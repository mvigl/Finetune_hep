{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from comet_ml import Experiment,ExistingExperiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "import yaml\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Finetune_hep.python import train,helpers,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = helpers.get_device()\n",
    "data_config_Xbb = 'config/ParT_Xbb_config.yaml'\n",
    "data_config_latent = 'config/ParT_latent_config.yaml'\n",
    "data_config_latent_hlf = 'config/ParT_latent_hlf_config.yaml'\n",
    "data_config_Xbb_hlf = 'config/ParT_Xbb_hlf_config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Xbb = models.full_model(data_config_Xbb,for_inference=True)  \n",
    "model_latent = models.full_model(data_config_latent,for_inference=False)  \n",
    "model_Xbb_hlf = models.full_model(data_config_Xbb_hlf,for_inference=True,save_representaions=True)  \n",
    "model_latent_hlf = models.full_model(data_config_latent_hlf,for_inference=True,save_representaions=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParticleTransformerWrapper(\n",
       "  (head): InvariantModel(\n",
       "    (phi): Sequential(\n",
       "      (0): Linear(in_features=133, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (rho): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (mod): ParticleTransformer(\n",
       "    (trimmer): SequenceTrimmer()\n",
       "    (embed): Embed(\n",
       "      (input_bn): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (embed): Sequential(\n",
       "        (0): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=17, out_features=128, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (7): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (8): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (pair_embed): PairEmbed(\n",
       "      (embed): Sequential(\n",
       "        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))\n",
       "        (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (cls_blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_latent_hlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParticleTransformerWrapper(\n",
       "  (Xbb): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (head): InvariantModel(\n",
       "    (phi): Sequential(\n",
       "      (0): Linear(in_features=6, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (rho): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (mod): ParticleTransformer(\n",
       "    (trimmer): SequenceTrimmer()\n",
       "    (embed): Embed(\n",
       "      (input_bn): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (embed): Sequential(\n",
       "        (0): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=17, out_features=128, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (7): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (8): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (pair_embed): PairEmbed(\n",
       "      (embed): Sequential(\n",
       "        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))\n",
       "        (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (cls_blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Xbb_hlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParticleTransformerWrapper(\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       "  (mod): ParticleTransformer(\n",
       "    (trimmer): SequenceTrimmer()\n",
       "    (embed): Embed(\n",
       "      (input_bn): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (embed): Sequential(\n",
       "        (0): LayerNorm((17,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=17, out_features=128, bias=True)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (7): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (8): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (pair_embed): PairEmbed(\n",
       "      (embed): Sequential(\n",
       "        (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
       "        (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): GELU(approximate='none')\n",
       "        (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): GELU(approximate='none')\n",
       "        (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "        (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (9): GELU(approximate='none')\n",
       "        (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))\n",
       "        (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (12): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-7): 8 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (cls_blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (pre_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (post_attn_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (pre_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (act_dropout): Dropout(p=0, inplace=False)\n",
       "        (post_fc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xbb'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Xbb.Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['X_jet', 'X_jet_singlejet', 'X_label', 'X_label_singlejet', 'X_pfo', 'X_pfo_singlejet', 'jet_mask', 'labels']>\n",
      "<HDF5 dataset \"X_jet\": shape (26, 5, 6), type \"<f8\">\n",
      "<HDF5 dataset \"X_jet_singlejet\": shape (60, 6), type \"<f8\">\n",
      "<HDF5 dataset \"X_pfo_singlejet\": shape (60, 100, 15), type \"<f8\">\n",
      "<KeysViewHDF5 ['X_jet', 'X_jet_singlejet', 'X_label', 'X_label_singlejet', 'X_pfo', 'X_pfo_singlejet', 'jet_mask', 'labels']>\n",
      "<HDF5 dataset \"X_jet\": shape (3536, 5, 6), type \"<f8\">\n",
      "<HDF5 dataset \"X_jet_singlejet\": shape (7975, 6), type \"<f8\">\n",
      "<HDF5 dataset \"X_pfo_singlejet\": shape (7975, 100, 15), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('../data/Data_val_sig_30.h5', 'r') as sample_sig:\n",
    "    data = {}\n",
    "    print(sample_sig.keys())\n",
    "    print(sample_sig['X_jet'])\n",
    "    print(sample_sig['X_jet_singlejet'])\n",
    "    print(sample_sig['X_pfo_singlejet'])\n",
    "    data['X_jet'] = sample_sig['X_jet'][:]\n",
    "    data['X_pfo'] = sample_sig['X_pfo'][:]\n",
    "    data['labels'] = sample_sig['labels'][:]\n",
    "    data['jet_mask'] = sample_sig['jet_mask'][:]\n",
    "    inputs = helpers.build_features_and_labels(data)\n",
    "    data_Xbb = {}\n",
    "    data_Xbb['X_jet'] = sample_sig['X_jet_singlejet'][:]\n",
    "    data_Xbb['X_pfo'] = sample_sig['X_pfo_singlejet'][:]\n",
    "    data_Xbb['labels'] = sample_sig['X_label_singlejet'][:]\n",
    "    inputs_Xbb = helpers.build_features_and_labels_Xbb(data_Xbb)\n",
    "with h5py.File('../data/Data_val_bkg_100.h5', 'r') as sample_bkg:\n",
    "    data_bkg = {}\n",
    "    print(sample_bkg.keys())\n",
    "    print(sample_bkg['X_jet'])\n",
    "    print(sample_bkg['X_jet_singlejet'])\n",
    "    print(sample_bkg['X_pfo_singlejet'])\n",
    "    data_bkg['X_jet'] = sample_bkg['X_jet'][:100]\n",
    "    data_bkg['X_pfo'] = sample_bkg['X_pfo'][:100]\n",
    "    data_bkg['labels'] = sample_bkg['labels'][:100]\n",
    "    data_bkg['jet_mask'] = sample_bkg['jet_mask'][:100]\n",
    "    inputs_bkg = helpers.build_features_and_labels(data_bkg)\n",
    "    data_Xbb_bkg = {}\n",
    "    data_Xbb_bkg['X_jet'] = sample_bkg['X_jet_singlejet'][:100]\n",
    "    data_Xbb_bkg['X_pfo'] = sample_bkg['X_pfo_singlejet'][:100]\n",
    "    data_Xbb_bkg['labels'] = sample_bkg['X_label_singlejet'][:100]\n",
    "    inputs_Xbb_bkg = helpers.build_features_and_labels_Xbb(data_Xbb_bkg)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pf_points', 'pf_features', 'pf_vectors', 'pf_mask', 'jet_mask', 'label', 'hl_feats'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_points = torch.tensor(inputs['pf_points']).float().to(device)\n",
    "pf_features = torch.tensor(inputs['pf_features']).float().to(device)\n",
    "pf_vectors = torch.tensor(inputs['pf_vectors']).float().to(device)\n",
    "pf_mask = torch.tensor(inputs['pf_mask']).float().to(device)\n",
    "hl_feats = torch.tensor(inputs['hl_feats']).float().to(device)\n",
    "jet_mask = torch.tensor(inputs['jet_mask']).float().to(device)\n",
    "\n",
    "preds = model_latent(pf_points,pf_features,pf_vectors,pf_mask,jet_mask,hl_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_points = torch.tensor(inputs_Xbb['pf_points']).float().to(device)\n",
    "pf_features = torch.tensor(inputs_Xbb['pf_features']).float().to(device)\n",
    "pf_vectors = torch.tensor(inputs_Xbb['pf_vectors']).float().to(device)\n",
    "pf_mask = torch.tensor(inputs_Xbb['pf_mask']).float().to(device)\n",
    "\n",
    "pf_points_bkg = torch.tensor(inputs_Xbb_bkg['pf_points']).float().to(device)\n",
    "pf_features_bkg = torch.tensor(inputs_Xbb_bkg['pf_features']).float().to(device)\n",
    "pf_vectors_bkg = torch.tensor(inputs_Xbb_bkg['pf_vectors']).float().to(device)\n",
    "pf_mask_bkg = torch.tensor(inputs_Xbb_bkg['pf_mask']).float().to(device)\n",
    "\n",
    "preds_Xbb = model_Xbb(pf_points,pf_features,pf_vectors,pf_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_Xbb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxmap = helpers.get_idxmap('config/train.txt')\n",
    "idxmap_val = helpers.get_idxmap('config/val.txt')\n",
    "integer_file_map = helpers.create_integer_file_map(idxmap)\n",
    "integer_file_map_val = helpers.create_integer_file_map(idxmap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_Xbb.load_state_dict(torch.load('/Users/matthiasvigl/Xbb_lr0.01_bs512_subset0.1.pt',map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights :\n",
      "dict_keys(['mod.cls_token', 'mod.embed.input_bn.weight', 'mod.embed.input_bn.bias', 'mod.embed.input_bn.running_mean', 'mod.embed.input_bn.running_var', 'mod.embed.input_bn.num_batches_tracked', 'mod.embed.embed.0.weight', 'mod.embed.embed.0.bias', 'mod.embed.embed.1.weight', 'mod.embed.embed.1.bias', 'mod.embed.embed.3.weight', 'mod.embed.embed.3.bias', 'mod.embed.embed.4.weight', 'mod.embed.embed.4.bias', 'mod.embed.embed.6.weight', 'mod.embed.embed.6.bias', 'mod.embed.embed.7.weight', 'mod.embed.embed.7.bias', 'mod.pair_embed.embed.0.weight', 'mod.pair_embed.embed.0.bias', 'mod.pair_embed.embed.0.running_mean', 'mod.pair_embed.embed.0.running_var', 'mod.pair_embed.embed.0.num_batches_tracked', 'mod.pair_embed.embed.1.weight', 'mod.pair_embed.embed.1.bias', 'mod.pair_embed.embed.2.weight', 'mod.pair_embed.embed.2.bias', 'mod.pair_embed.embed.2.running_mean', 'mod.pair_embed.embed.2.running_var', 'mod.pair_embed.embed.2.num_batches_tracked', 'mod.pair_embed.embed.4.weight', 'mod.pair_embed.embed.4.bias', 'mod.pair_embed.embed.5.weight', 'mod.pair_embed.embed.5.bias', 'mod.pair_embed.embed.5.running_mean', 'mod.pair_embed.embed.5.running_var', 'mod.pair_embed.embed.5.num_batches_tracked', 'mod.pair_embed.embed.7.weight', 'mod.pair_embed.embed.7.bias', 'mod.pair_embed.embed.8.weight', 'mod.pair_embed.embed.8.bias', 'mod.pair_embed.embed.8.running_mean', 'mod.pair_embed.embed.8.running_var', 'mod.pair_embed.embed.8.num_batches_tracked', 'mod.pair_embed.embed.10.weight', 'mod.pair_embed.embed.10.bias', 'mod.pair_embed.embed.11.weight', 'mod.pair_embed.embed.11.bias', 'mod.pair_embed.embed.11.running_mean', 'mod.pair_embed.embed.11.running_var', 'mod.pair_embed.embed.11.num_batches_tracked', 'mod.blocks.0.c_attn', 'mod.blocks.0.w_resid', 'mod.blocks.0.pre_attn_norm.weight', 'mod.blocks.0.pre_attn_norm.bias', 'mod.blocks.0.attn.in_proj_weight', 'mod.blocks.0.attn.in_proj_bias', 'mod.blocks.0.attn.out_proj.weight', 'mod.blocks.0.attn.out_proj.bias', 'mod.blocks.0.post_attn_norm.weight', 'mod.blocks.0.post_attn_norm.bias', 'mod.blocks.0.pre_fc_norm.weight', 'mod.blocks.0.pre_fc_norm.bias', 'mod.blocks.0.fc1.weight', 'mod.blocks.0.fc1.bias', 'mod.blocks.0.post_fc_norm.weight', 'mod.blocks.0.post_fc_norm.bias', 'mod.blocks.0.fc2.weight', 'mod.blocks.0.fc2.bias', 'mod.blocks.1.c_attn', 'mod.blocks.1.w_resid', 'mod.blocks.1.pre_attn_norm.weight', 'mod.blocks.1.pre_attn_norm.bias', 'mod.blocks.1.attn.in_proj_weight', 'mod.blocks.1.attn.in_proj_bias', 'mod.blocks.1.attn.out_proj.weight', 'mod.blocks.1.attn.out_proj.bias', 'mod.blocks.1.post_attn_norm.weight', 'mod.blocks.1.post_attn_norm.bias', 'mod.blocks.1.pre_fc_norm.weight', 'mod.blocks.1.pre_fc_norm.bias', 'mod.blocks.1.fc1.weight', 'mod.blocks.1.fc1.bias', 'mod.blocks.1.post_fc_norm.weight', 'mod.blocks.1.post_fc_norm.bias', 'mod.blocks.1.fc2.weight', 'mod.blocks.1.fc2.bias', 'mod.blocks.2.c_attn', 'mod.blocks.2.w_resid', 'mod.blocks.2.pre_attn_norm.weight', 'mod.blocks.2.pre_attn_norm.bias', 'mod.blocks.2.attn.in_proj_weight', 'mod.blocks.2.attn.in_proj_bias', 'mod.blocks.2.attn.out_proj.weight', 'mod.blocks.2.attn.out_proj.bias', 'mod.blocks.2.post_attn_norm.weight', 'mod.blocks.2.post_attn_norm.bias', 'mod.blocks.2.pre_fc_norm.weight', 'mod.blocks.2.pre_fc_norm.bias', 'mod.blocks.2.fc1.weight', 'mod.blocks.2.fc1.bias', 'mod.blocks.2.post_fc_norm.weight', 'mod.blocks.2.post_fc_norm.bias', 'mod.blocks.2.fc2.weight', 'mod.blocks.2.fc2.bias', 'mod.blocks.3.c_attn', 'mod.blocks.3.w_resid', 'mod.blocks.3.pre_attn_norm.weight', 'mod.blocks.3.pre_attn_norm.bias', 'mod.blocks.3.attn.in_proj_weight', 'mod.blocks.3.attn.in_proj_bias', 'mod.blocks.3.attn.out_proj.weight', 'mod.blocks.3.attn.out_proj.bias', 'mod.blocks.3.post_attn_norm.weight', 'mod.blocks.3.post_attn_norm.bias', 'mod.blocks.3.pre_fc_norm.weight', 'mod.blocks.3.pre_fc_norm.bias', 'mod.blocks.3.fc1.weight', 'mod.blocks.3.fc1.bias', 'mod.blocks.3.post_fc_norm.weight', 'mod.blocks.3.post_fc_norm.bias', 'mod.blocks.3.fc2.weight', 'mod.blocks.3.fc2.bias', 'mod.blocks.4.c_attn', 'mod.blocks.4.w_resid', 'mod.blocks.4.pre_attn_norm.weight', 'mod.blocks.4.pre_attn_norm.bias', 'mod.blocks.4.attn.in_proj_weight', 'mod.blocks.4.attn.in_proj_bias', 'mod.blocks.4.attn.out_proj.weight', 'mod.blocks.4.attn.out_proj.bias', 'mod.blocks.4.post_attn_norm.weight', 'mod.blocks.4.post_attn_norm.bias', 'mod.blocks.4.pre_fc_norm.weight', 'mod.blocks.4.pre_fc_norm.bias', 'mod.blocks.4.fc1.weight', 'mod.blocks.4.fc1.bias', 'mod.blocks.4.post_fc_norm.weight', 'mod.blocks.4.post_fc_norm.bias', 'mod.blocks.4.fc2.weight', 'mod.blocks.4.fc2.bias', 'mod.blocks.5.c_attn', 'mod.blocks.5.w_resid', 'mod.blocks.5.pre_attn_norm.weight', 'mod.blocks.5.pre_attn_norm.bias', 'mod.blocks.5.attn.in_proj_weight', 'mod.blocks.5.attn.in_proj_bias', 'mod.blocks.5.attn.out_proj.weight', 'mod.blocks.5.attn.out_proj.bias', 'mod.blocks.5.post_attn_norm.weight', 'mod.blocks.5.post_attn_norm.bias', 'mod.blocks.5.pre_fc_norm.weight', 'mod.blocks.5.pre_fc_norm.bias', 'mod.blocks.5.fc1.weight', 'mod.blocks.5.fc1.bias', 'mod.blocks.5.post_fc_norm.weight', 'mod.blocks.5.post_fc_norm.bias', 'mod.blocks.5.fc2.weight', 'mod.blocks.5.fc2.bias', 'mod.blocks.6.c_attn', 'mod.blocks.6.w_resid', 'mod.blocks.6.pre_attn_norm.weight', 'mod.blocks.6.pre_attn_norm.bias', 'mod.blocks.6.attn.in_proj_weight', 'mod.blocks.6.attn.in_proj_bias', 'mod.blocks.6.attn.out_proj.weight', 'mod.blocks.6.attn.out_proj.bias', 'mod.blocks.6.post_attn_norm.weight', 'mod.blocks.6.post_attn_norm.bias', 'mod.blocks.6.pre_fc_norm.weight', 'mod.blocks.6.pre_fc_norm.bias', 'mod.blocks.6.fc1.weight', 'mod.blocks.6.fc1.bias', 'mod.blocks.6.post_fc_norm.weight', 'mod.blocks.6.post_fc_norm.bias', 'mod.blocks.6.fc2.weight', 'mod.blocks.6.fc2.bias', 'mod.blocks.7.c_attn', 'mod.blocks.7.w_resid', 'mod.blocks.7.pre_attn_norm.weight', 'mod.blocks.7.pre_attn_norm.bias', 'mod.blocks.7.attn.in_proj_weight', 'mod.blocks.7.attn.in_proj_bias', 'mod.blocks.7.attn.out_proj.weight', 'mod.blocks.7.attn.out_proj.bias', 'mod.blocks.7.post_attn_norm.weight', 'mod.blocks.7.post_attn_norm.bias', 'mod.blocks.7.pre_fc_norm.weight', 'mod.blocks.7.pre_fc_norm.bias', 'mod.blocks.7.fc1.weight', 'mod.blocks.7.fc1.bias', 'mod.blocks.7.post_fc_norm.weight', 'mod.blocks.7.post_fc_norm.bias', 'mod.blocks.7.fc2.weight', 'mod.blocks.7.fc2.bias', 'mod.cls_blocks.0.c_attn', 'mod.cls_blocks.0.w_resid', 'mod.cls_blocks.0.pre_attn_norm.weight', 'mod.cls_blocks.0.pre_attn_norm.bias', 'mod.cls_blocks.0.attn.in_proj_weight', 'mod.cls_blocks.0.attn.in_proj_bias', 'mod.cls_blocks.0.attn.out_proj.weight', 'mod.cls_blocks.0.attn.out_proj.bias', 'mod.cls_blocks.0.post_attn_norm.weight', 'mod.cls_blocks.0.post_attn_norm.bias', 'mod.cls_blocks.0.pre_fc_norm.weight', 'mod.cls_blocks.0.pre_fc_norm.bias', 'mod.cls_blocks.0.fc1.weight', 'mod.cls_blocks.0.fc1.bias', 'mod.cls_blocks.0.post_fc_norm.weight', 'mod.cls_blocks.0.post_fc_norm.bias', 'mod.cls_blocks.0.fc2.weight', 'mod.cls_blocks.0.fc2.bias', 'mod.cls_blocks.1.c_attn', 'mod.cls_blocks.1.w_resid', 'mod.cls_blocks.1.pre_attn_norm.weight', 'mod.cls_blocks.1.pre_attn_norm.bias', 'mod.cls_blocks.1.attn.in_proj_weight', 'mod.cls_blocks.1.attn.in_proj_bias', 'mod.cls_blocks.1.attn.out_proj.weight', 'mod.cls_blocks.1.attn.out_proj.bias', 'mod.cls_blocks.1.post_attn_norm.weight', 'mod.cls_blocks.1.post_attn_norm.bias', 'mod.cls_blocks.1.pre_fc_norm.weight', 'mod.cls_blocks.1.pre_fc_norm.bias', 'mod.cls_blocks.1.fc1.weight', 'mod.cls_blocks.1.fc1.bias', 'mod.cls_blocks.1.post_fc_norm.weight', 'mod.cls_blocks.1.post_fc_norm.bias', 'mod.cls_blocks.1.fc2.weight', 'mod.cls_blocks.1.fc2.bias', 'mod.norm.weight', 'mod.norm.bias'])\n",
      "loading weights :\n",
      "dict_keys(['mod.cls_token', 'mod.embed.input_bn.weight', 'mod.embed.input_bn.bias', 'mod.embed.input_bn.running_mean', 'mod.embed.input_bn.running_var', 'mod.embed.input_bn.num_batches_tracked', 'mod.embed.embed.0.weight', 'mod.embed.embed.0.bias', 'mod.embed.embed.1.weight', 'mod.embed.embed.1.bias', 'mod.embed.embed.3.weight', 'mod.embed.embed.3.bias', 'mod.embed.embed.4.weight', 'mod.embed.embed.4.bias', 'mod.embed.embed.6.weight', 'mod.embed.embed.6.bias', 'mod.embed.embed.7.weight', 'mod.embed.embed.7.bias', 'mod.pair_embed.embed.0.weight', 'mod.pair_embed.embed.0.bias', 'mod.pair_embed.embed.0.running_mean', 'mod.pair_embed.embed.0.running_var', 'mod.pair_embed.embed.0.num_batches_tracked', 'mod.pair_embed.embed.1.weight', 'mod.pair_embed.embed.1.bias', 'mod.pair_embed.embed.2.weight', 'mod.pair_embed.embed.2.bias', 'mod.pair_embed.embed.2.running_mean', 'mod.pair_embed.embed.2.running_var', 'mod.pair_embed.embed.2.num_batches_tracked', 'mod.pair_embed.embed.4.weight', 'mod.pair_embed.embed.4.bias', 'mod.pair_embed.embed.5.weight', 'mod.pair_embed.embed.5.bias', 'mod.pair_embed.embed.5.running_mean', 'mod.pair_embed.embed.5.running_var', 'mod.pair_embed.embed.5.num_batches_tracked', 'mod.pair_embed.embed.7.weight', 'mod.pair_embed.embed.7.bias', 'mod.pair_embed.embed.8.weight', 'mod.pair_embed.embed.8.bias', 'mod.pair_embed.embed.8.running_mean', 'mod.pair_embed.embed.8.running_var', 'mod.pair_embed.embed.8.num_batches_tracked', 'mod.pair_embed.embed.10.weight', 'mod.pair_embed.embed.10.bias', 'mod.pair_embed.embed.11.weight', 'mod.pair_embed.embed.11.bias', 'mod.pair_embed.embed.11.running_mean', 'mod.pair_embed.embed.11.running_var', 'mod.pair_embed.embed.11.num_batches_tracked', 'mod.blocks.0.c_attn', 'mod.blocks.0.w_resid', 'mod.blocks.0.pre_attn_norm.weight', 'mod.blocks.0.pre_attn_norm.bias', 'mod.blocks.0.attn.in_proj_weight', 'mod.blocks.0.attn.in_proj_bias', 'mod.blocks.0.attn.out_proj.weight', 'mod.blocks.0.attn.out_proj.bias', 'mod.blocks.0.post_attn_norm.weight', 'mod.blocks.0.post_attn_norm.bias', 'mod.blocks.0.pre_fc_norm.weight', 'mod.blocks.0.pre_fc_norm.bias', 'mod.blocks.0.fc1.weight', 'mod.blocks.0.fc1.bias', 'mod.blocks.0.post_fc_norm.weight', 'mod.blocks.0.post_fc_norm.bias', 'mod.blocks.0.fc2.weight', 'mod.blocks.0.fc2.bias', 'mod.blocks.1.c_attn', 'mod.blocks.1.w_resid', 'mod.blocks.1.pre_attn_norm.weight', 'mod.blocks.1.pre_attn_norm.bias', 'mod.blocks.1.attn.in_proj_weight', 'mod.blocks.1.attn.in_proj_bias', 'mod.blocks.1.attn.out_proj.weight', 'mod.blocks.1.attn.out_proj.bias', 'mod.blocks.1.post_attn_norm.weight', 'mod.blocks.1.post_attn_norm.bias', 'mod.blocks.1.pre_fc_norm.weight', 'mod.blocks.1.pre_fc_norm.bias', 'mod.blocks.1.fc1.weight', 'mod.blocks.1.fc1.bias', 'mod.blocks.1.post_fc_norm.weight', 'mod.blocks.1.post_fc_norm.bias', 'mod.blocks.1.fc2.weight', 'mod.blocks.1.fc2.bias', 'mod.blocks.2.c_attn', 'mod.blocks.2.w_resid', 'mod.blocks.2.pre_attn_norm.weight', 'mod.blocks.2.pre_attn_norm.bias', 'mod.blocks.2.attn.in_proj_weight', 'mod.blocks.2.attn.in_proj_bias', 'mod.blocks.2.attn.out_proj.weight', 'mod.blocks.2.attn.out_proj.bias', 'mod.blocks.2.post_attn_norm.weight', 'mod.blocks.2.post_attn_norm.bias', 'mod.blocks.2.pre_fc_norm.weight', 'mod.blocks.2.pre_fc_norm.bias', 'mod.blocks.2.fc1.weight', 'mod.blocks.2.fc1.bias', 'mod.blocks.2.post_fc_norm.weight', 'mod.blocks.2.post_fc_norm.bias', 'mod.blocks.2.fc2.weight', 'mod.blocks.2.fc2.bias', 'mod.blocks.3.c_attn', 'mod.blocks.3.w_resid', 'mod.blocks.3.pre_attn_norm.weight', 'mod.blocks.3.pre_attn_norm.bias', 'mod.blocks.3.attn.in_proj_weight', 'mod.blocks.3.attn.in_proj_bias', 'mod.blocks.3.attn.out_proj.weight', 'mod.blocks.3.attn.out_proj.bias', 'mod.blocks.3.post_attn_norm.weight', 'mod.blocks.3.post_attn_norm.bias', 'mod.blocks.3.pre_fc_norm.weight', 'mod.blocks.3.pre_fc_norm.bias', 'mod.blocks.3.fc1.weight', 'mod.blocks.3.fc1.bias', 'mod.blocks.3.post_fc_norm.weight', 'mod.blocks.3.post_fc_norm.bias', 'mod.blocks.3.fc2.weight', 'mod.blocks.3.fc2.bias', 'mod.blocks.4.c_attn', 'mod.blocks.4.w_resid', 'mod.blocks.4.pre_attn_norm.weight', 'mod.blocks.4.pre_attn_norm.bias', 'mod.blocks.4.attn.in_proj_weight', 'mod.blocks.4.attn.in_proj_bias', 'mod.blocks.4.attn.out_proj.weight', 'mod.blocks.4.attn.out_proj.bias', 'mod.blocks.4.post_attn_norm.weight', 'mod.blocks.4.post_attn_norm.bias', 'mod.blocks.4.pre_fc_norm.weight', 'mod.blocks.4.pre_fc_norm.bias', 'mod.blocks.4.fc1.weight', 'mod.blocks.4.fc1.bias', 'mod.blocks.4.post_fc_norm.weight', 'mod.blocks.4.post_fc_norm.bias', 'mod.blocks.4.fc2.weight', 'mod.blocks.4.fc2.bias', 'mod.blocks.5.c_attn', 'mod.blocks.5.w_resid', 'mod.blocks.5.pre_attn_norm.weight', 'mod.blocks.5.pre_attn_norm.bias', 'mod.blocks.5.attn.in_proj_weight', 'mod.blocks.5.attn.in_proj_bias', 'mod.blocks.5.attn.out_proj.weight', 'mod.blocks.5.attn.out_proj.bias', 'mod.blocks.5.post_attn_norm.weight', 'mod.blocks.5.post_attn_norm.bias', 'mod.blocks.5.pre_fc_norm.weight', 'mod.blocks.5.pre_fc_norm.bias', 'mod.blocks.5.fc1.weight', 'mod.blocks.5.fc1.bias', 'mod.blocks.5.post_fc_norm.weight', 'mod.blocks.5.post_fc_norm.bias', 'mod.blocks.5.fc2.weight', 'mod.blocks.5.fc2.bias', 'mod.blocks.6.c_attn', 'mod.blocks.6.w_resid', 'mod.blocks.6.pre_attn_norm.weight', 'mod.blocks.6.pre_attn_norm.bias', 'mod.blocks.6.attn.in_proj_weight', 'mod.blocks.6.attn.in_proj_bias', 'mod.blocks.6.attn.out_proj.weight', 'mod.blocks.6.attn.out_proj.bias', 'mod.blocks.6.post_attn_norm.weight', 'mod.blocks.6.post_attn_norm.bias', 'mod.blocks.6.pre_fc_norm.weight', 'mod.blocks.6.pre_fc_norm.bias', 'mod.blocks.6.fc1.weight', 'mod.blocks.6.fc1.bias', 'mod.blocks.6.post_fc_norm.weight', 'mod.blocks.6.post_fc_norm.bias', 'mod.blocks.6.fc2.weight', 'mod.blocks.6.fc2.bias', 'mod.blocks.7.c_attn', 'mod.blocks.7.w_resid', 'mod.blocks.7.pre_attn_norm.weight', 'mod.blocks.7.pre_attn_norm.bias', 'mod.blocks.7.attn.in_proj_weight', 'mod.blocks.7.attn.in_proj_bias', 'mod.blocks.7.attn.out_proj.weight', 'mod.blocks.7.attn.out_proj.bias', 'mod.blocks.7.post_attn_norm.weight', 'mod.blocks.7.post_attn_norm.bias', 'mod.blocks.7.pre_fc_norm.weight', 'mod.blocks.7.pre_fc_norm.bias', 'mod.blocks.7.fc1.weight', 'mod.blocks.7.fc1.bias', 'mod.blocks.7.post_fc_norm.weight', 'mod.blocks.7.post_fc_norm.bias', 'mod.blocks.7.fc2.weight', 'mod.blocks.7.fc2.bias', 'mod.cls_blocks.0.c_attn', 'mod.cls_blocks.0.w_resid', 'mod.cls_blocks.0.pre_attn_norm.weight', 'mod.cls_blocks.0.pre_attn_norm.bias', 'mod.cls_blocks.0.attn.in_proj_weight', 'mod.cls_blocks.0.attn.in_proj_bias', 'mod.cls_blocks.0.attn.out_proj.weight', 'mod.cls_blocks.0.attn.out_proj.bias', 'mod.cls_blocks.0.post_attn_norm.weight', 'mod.cls_blocks.0.post_attn_norm.bias', 'mod.cls_blocks.0.pre_fc_norm.weight', 'mod.cls_blocks.0.pre_fc_norm.bias', 'mod.cls_blocks.0.fc1.weight', 'mod.cls_blocks.0.fc1.bias', 'mod.cls_blocks.0.post_fc_norm.weight', 'mod.cls_blocks.0.post_fc_norm.bias', 'mod.cls_blocks.0.fc2.weight', 'mod.cls_blocks.0.fc2.bias', 'mod.cls_blocks.1.c_attn', 'mod.cls_blocks.1.w_resid', 'mod.cls_blocks.1.pre_attn_norm.weight', 'mod.cls_blocks.1.pre_attn_norm.bias', 'mod.cls_blocks.1.attn.in_proj_weight', 'mod.cls_blocks.1.attn.in_proj_bias', 'mod.cls_blocks.1.attn.out_proj.weight', 'mod.cls_blocks.1.attn.out_proj.bias', 'mod.cls_blocks.1.post_attn_norm.weight', 'mod.cls_blocks.1.post_attn_norm.bias', 'mod.cls_blocks.1.pre_fc_norm.weight', 'mod.cls_blocks.1.pre_fc_norm.bias', 'mod.cls_blocks.1.fc1.weight', 'mod.cls_blocks.1.fc1.bias', 'mod.cls_blocks.1.post_fc_norm.weight', 'mod.cls_blocks.1.post_fc_norm.bias', 'mod.cls_blocks.1.fc2.weight', 'mod.cls_blocks.1.fc2.bias', 'mod.norm.weight', 'mod.norm.bias'])\n",
      "loading weights :\n",
      "dict_keys(['Xbb.0.weight', 'Xbb.0.bias', 'mod.cls_token', 'mod.embed.input_bn.weight', 'mod.embed.input_bn.bias', 'mod.embed.input_bn.running_mean', 'mod.embed.input_bn.running_var', 'mod.embed.input_bn.num_batches_tracked', 'mod.embed.embed.0.weight', 'mod.embed.embed.0.bias', 'mod.embed.embed.1.weight', 'mod.embed.embed.1.bias', 'mod.embed.embed.3.weight', 'mod.embed.embed.3.bias', 'mod.embed.embed.4.weight', 'mod.embed.embed.4.bias', 'mod.embed.embed.6.weight', 'mod.embed.embed.6.bias', 'mod.embed.embed.7.weight', 'mod.embed.embed.7.bias', 'mod.pair_embed.embed.0.weight', 'mod.pair_embed.embed.0.bias', 'mod.pair_embed.embed.0.running_mean', 'mod.pair_embed.embed.0.running_var', 'mod.pair_embed.embed.0.num_batches_tracked', 'mod.pair_embed.embed.1.weight', 'mod.pair_embed.embed.1.bias', 'mod.pair_embed.embed.2.weight', 'mod.pair_embed.embed.2.bias', 'mod.pair_embed.embed.2.running_mean', 'mod.pair_embed.embed.2.running_var', 'mod.pair_embed.embed.2.num_batches_tracked', 'mod.pair_embed.embed.4.weight', 'mod.pair_embed.embed.4.bias', 'mod.pair_embed.embed.5.weight', 'mod.pair_embed.embed.5.bias', 'mod.pair_embed.embed.5.running_mean', 'mod.pair_embed.embed.5.running_var', 'mod.pair_embed.embed.5.num_batches_tracked', 'mod.pair_embed.embed.7.weight', 'mod.pair_embed.embed.7.bias', 'mod.pair_embed.embed.8.weight', 'mod.pair_embed.embed.8.bias', 'mod.pair_embed.embed.8.running_mean', 'mod.pair_embed.embed.8.running_var', 'mod.pair_embed.embed.8.num_batches_tracked', 'mod.pair_embed.embed.10.weight', 'mod.pair_embed.embed.10.bias', 'mod.pair_embed.embed.11.weight', 'mod.pair_embed.embed.11.bias', 'mod.pair_embed.embed.11.running_mean', 'mod.pair_embed.embed.11.running_var', 'mod.pair_embed.embed.11.num_batches_tracked', 'mod.blocks.0.c_attn', 'mod.blocks.0.w_resid', 'mod.blocks.0.pre_attn_norm.weight', 'mod.blocks.0.pre_attn_norm.bias', 'mod.blocks.0.attn.in_proj_weight', 'mod.blocks.0.attn.in_proj_bias', 'mod.blocks.0.attn.out_proj.weight', 'mod.blocks.0.attn.out_proj.bias', 'mod.blocks.0.post_attn_norm.weight', 'mod.blocks.0.post_attn_norm.bias', 'mod.blocks.0.pre_fc_norm.weight', 'mod.blocks.0.pre_fc_norm.bias', 'mod.blocks.0.fc1.weight', 'mod.blocks.0.fc1.bias', 'mod.blocks.0.post_fc_norm.weight', 'mod.blocks.0.post_fc_norm.bias', 'mod.blocks.0.fc2.weight', 'mod.blocks.0.fc2.bias', 'mod.blocks.1.c_attn', 'mod.blocks.1.w_resid', 'mod.blocks.1.pre_attn_norm.weight', 'mod.blocks.1.pre_attn_norm.bias', 'mod.blocks.1.attn.in_proj_weight', 'mod.blocks.1.attn.in_proj_bias', 'mod.blocks.1.attn.out_proj.weight', 'mod.blocks.1.attn.out_proj.bias', 'mod.blocks.1.post_attn_norm.weight', 'mod.blocks.1.post_attn_norm.bias', 'mod.blocks.1.pre_fc_norm.weight', 'mod.blocks.1.pre_fc_norm.bias', 'mod.blocks.1.fc1.weight', 'mod.blocks.1.fc1.bias', 'mod.blocks.1.post_fc_norm.weight', 'mod.blocks.1.post_fc_norm.bias', 'mod.blocks.1.fc2.weight', 'mod.blocks.1.fc2.bias', 'mod.blocks.2.c_attn', 'mod.blocks.2.w_resid', 'mod.blocks.2.pre_attn_norm.weight', 'mod.blocks.2.pre_attn_norm.bias', 'mod.blocks.2.attn.in_proj_weight', 'mod.blocks.2.attn.in_proj_bias', 'mod.blocks.2.attn.out_proj.weight', 'mod.blocks.2.attn.out_proj.bias', 'mod.blocks.2.post_attn_norm.weight', 'mod.blocks.2.post_attn_norm.bias', 'mod.blocks.2.pre_fc_norm.weight', 'mod.blocks.2.pre_fc_norm.bias', 'mod.blocks.2.fc1.weight', 'mod.blocks.2.fc1.bias', 'mod.blocks.2.post_fc_norm.weight', 'mod.blocks.2.post_fc_norm.bias', 'mod.blocks.2.fc2.weight', 'mod.blocks.2.fc2.bias', 'mod.blocks.3.c_attn', 'mod.blocks.3.w_resid', 'mod.blocks.3.pre_attn_norm.weight', 'mod.blocks.3.pre_attn_norm.bias', 'mod.blocks.3.attn.in_proj_weight', 'mod.blocks.3.attn.in_proj_bias', 'mod.blocks.3.attn.out_proj.weight', 'mod.blocks.3.attn.out_proj.bias', 'mod.blocks.3.post_attn_norm.weight', 'mod.blocks.3.post_attn_norm.bias', 'mod.blocks.3.pre_fc_norm.weight', 'mod.blocks.3.pre_fc_norm.bias', 'mod.blocks.3.fc1.weight', 'mod.blocks.3.fc1.bias', 'mod.blocks.3.post_fc_norm.weight', 'mod.blocks.3.post_fc_norm.bias', 'mod.blocks.3.fc2.weight', 'mod.blocks.3.fc2.bias', 'mod.blocks.4.c_attn', 'mod.blocks.4.w_resid', 'mod.blocks.4.pre_attn_norm.weight', 'mod.blocks.4.pre_attn_norm.bias', 'mod.blocks.4.attn.in_proj_weight', 'mod.blocks.4.attn.in_proj_bias', 'mod.blocks.4.attn.out_proj.weight', 'mod.blocks.4.attn.out_proj.bias', 'mod.blocks.4.post_attn_norm.weight', 'mod.blocks.4.post_attn_norm.bias', 'mod.blocks.4.pre_fc_norm.weight', 'mod.blocks.4.pre_fc_norm.bias', 'mod.blocks.4.fc1.weight', 'mod.blocks.4.fc1.bias', 'mod.blocks.4.post_fc_norm.weight', 'mod.blocks.4.post_fc_norm.bias', 'mod.blocks.4.fc2.weight', 'mod.blocks.4.fc2.bias', 'mod.blocks.5.c_attn', 'mod.blocks.5.w_resid', 'mod.blocks.5.pre_attn_norm.weight', 'mod.blocks.5.pre_attn_norm.bias', 'mod.blocks.5.attn.in_proj_weight', 'mod.blocks.5.attn.in_proj_bias', 'mod.blocks.5.attn.out_proj.weight', 'mod.blocks.5.attn.out_proj.bias', 'mod.blocks.5.post_attn_norm.weight', 'mod.blocks.5.post_attn_norm.bias', 'mod.blocks.5.pre_fc_norm.weight', 'mod.blocks.5.pre_fc_norm.bias', 'mod.blocks.5.fc1.weight', 'mod.blocks.5.fc1.bias', 'mod.blocks.5.post_fc_norm.weight', 'mod.blocks.5.post_fc_norm.bias', 'mod.blocks.5.fc2.weight', 'mod.blocks.5.fc2.bias', 'mod.blocks.6.c_attn', 'mod.blocks.6.w_resid', 'mod.blocks.6.pre_attn_norm.weight', 'mod.blocks.6.pre_attn_norm.bias', 'mod.blocks.6.attn.in_proj_weight', 'mod.blocks.6.attn.in_proj_bias', 'mod.blocks.6.attn.out_proj.weight', 'mod.blocks.6.attn.out_proj.bias', 'mod.blocks.6.post_attn_norm.weight', 'mod.blocks.6.post_attn_norm.bias', 'mod.blocks.6.pre_fc_norm.weight', 'mod.blocks.6.pre_fc_norm.bias', 'mod.blocks.6.fc1.weight', 'mod.blocks.6.fc1.bias', 'mod.blocks.6.post_fc_norm.weight', 'mod.blocks.6.post_fc_norm.bias', 'mod.blocks.6.fc2.weight', 'mod.blocks.6.fc2.bias', 'mod.blocks.7.c_attn', 'mod.blocks.7.w_resid', 'mod.blocks.7.pre_attn_norm.weight', 'mod.blocks.7.pre_attn_norm.bias', 'mod.blocks.7.attn.in_proj_weight', 'mod.blocks.7.attn.in_proj_bias', 'mod.blocks.7.attn.out_proj.weight', 'mod.blocks.7.attn.out_proj.bias', 'mod.blocks.7.post_attn_norm.weight', 'mod.blocks.7.post_attn_norm.bias', 'mod.blocks.7.pre_fc_norm.weight', 'mod.blocks.7.pre_fc_norm.bias', 'mod.blocks.7.fc1.weight', 'mod.blocks.7.fc1.bias', 'mod.blocks.7.post_fc_norm.weight', 'mod.blocks.7.post_fc_norm.bias', 'mod.blocks.7.fc2.weight', 'mod.blocks.7.fc2.bias', 'mod.cls_blocks.0.c_attn', 'mod.cls_blocks.0.w_resid', 'mod.cls_blocks.0.pre_attn_norm.weight', 'mod.cls_blocks.0.pre_attn_norm.bias', 'mod.cls_blocks.0.attn.in_proj_weight', 'mod.cls_blocks.0.attn.in_proj_bias', 'mod.cls_blocks.0.attn.out_proj.weight', 'mod.cls_blocks.0.attn.out_proj.bias', 'mod.cls_blocks.0.post_attn_norm.weight', 'mod.cls_blocks.0.post_attn_norm.bias', 'mod.cls_blocks.0.pre_fc_norm.weight', 'mod.cls_blocks.0.pre_fc_norm.bias', 'mod.cls_blocks.0.fc1.weight', 'mod.cls_blocks.0.fc1.bias', 'mod.cls_blocks.0.post_fc_norm.weight', 'mod.cls_blocks.0.post_fc_norm.bias', 'mod.cls_blocks.0.fc2.weight', 'mod.cls_blocks.0.fc2.bias', 'mod.cls_blocks.1.c_attn', 'mod.cls_blocks.1.w_resid', 'mod.cls_blocks.1.pre_attn_norm.weight', 'mod.cls_blocks.1.pre_attn_norm.bias', 'mod.cls_blocks.1.attn.in_proj_weight', 'mod.cls_blocks.1.attn.in_proj_bias', 'mod.cls_blocks.1.attn.out_proj.weight', 'mod.cls_blocks.1.attn.out_proj.bias', 'mod.cls_blocks.1.post_attn_norm.weight', 'mod.cls_blocks.1.post_attn_norm.bias', 'mod.cls_blocks.1.pre_fc_norm.weight', 'mod.cls_blocks.1.pre_fc_norm.bias', 'mod.cls_blocks.1.fc1.weight', 'mod.cls_blocks.1.fc1.bias', 'mod.cls_blocks.1.post_fc_norm.weight', 'mod.cls_blocks.1.post_fc_norm.bias', 'mod.cls_blocks.1.fc2.weight', 'mod.cls_blocks.1.fc2.bias', 'mod.norm.weight', 'mod.norm.bias'])\n",
      "loading weights :\n",
      "dict_keys(['Xbb.0.weight', 'Xbb.0.bias', 'mod.cls_token', 'mod.embed.input_bn.weight', 'mod.embed.input_bn.bias', 'mod.embed.input_bn.running_mean', 'mod.embed.input_bn.running_var', 'mod.embed.input_bn.num_batches_tracked', 'mod.embed.embed.0.weight', 'mod.embed.embed.0.bias', 'mod.embed.embed.1.weight', 'mod.embed.embed.1.bias', 'mod.embed.embed.3.weight', 'mod.embed.embed.3.bias', 'mod.embed.embed.4.weight', 'mod.embed.embed.4.bias', 'mod.embed.embed.6.weight', 'mod.embed.embed.6.bias', 'mod.embed.embed.7.weight', 'mod.embed.embed.7.bias', 'mod.pair_embed.embed.0.weight', 'mod.pair_embed.embed.0.bias', 'mod.pair_embed.embed.0.running_mean', 'mod.pair_embed.embed.0.running_var', 'mod.pair_embed.embed.0.num_batches_tracked', 'mod.pair_embed.embed.1.weight', 'mod.pair_embed.embed.1.bias', 'mod.pair_embed.embed.2.weight', 'mod.pair_embed.embed.2.bias', 'mod.pair_embed.embed.2.running_mean', 'mod.pair_embed.embed.2.running_var', 'mod.pair_embed.embed.2.num_batches_tracked', 'mod.pair_embed.embed.4.weight', 'mod.pair_embed.embed.4.bias', 'mod.pair_embed.embed.5.weight', 'mod.pair_embed.embed.5.bias', 'mod.pair_embed.embed.5.running_mean', 'mod.pair_embed.embed.5.running_var', 'mod.pair_embed.embed.5.num_batches_tracked', 'mod.pair_embed.embed.7.weight', 'mod.pair_embed.embed.7.bias', 'mod.pair_embed.embed.8.weight', 'mod.pair_embed.embed.8.bias', 'mod.pair_embed.embed.8.running_mean', 'mod.pair_embed.embed.8.running_var', 'mod.pair_embed.embed.8.num_batches_tracked', 'mod.pair_embed.embed.10.weight', 'mod.pair_embed.embed.10.bias', 'mod.pair_embed.embed.11.weight', 'mod.pair_embed.embed.11.bias', 'mod.pair_embed.embed.11.running_mean', 'mod.pair_embed.embed.11.running_var', 'mod.pair_embed.embed.11.num_batches_tracked', 'mod.blocks.0.c_attn', 'mod.blocks.0.w_resid', 'mod.blocks.0.pre_attn_norm.weight', 'mod.blocks.0.pre_attn_norm.bias', 'mod.blocks.0.attn.in_proj_weight', 'mod.blocks.0.attn.in_proj_bias', 'mod.blocks.0.attn.out_proj.weight', 'mod.blocks.0.attn.out_proj.bias', 'mod.blocks.0.post_attn_norm.weight', 'mod.blocks.0.post_attn_norm.bias', 'mod.blocks.0.pre_fc_norm.weight', 'mod.blocks.0.pre_fc_norm.bias', 'mod.blocks.0.fc1.weight', 'mod.blocks.0.fc1.bias', 'mod.blocks.0.post_fc_norm.weight', 'mod.blocks.0.post_fc_norm.bias', 'mod.blocks.0.fc2.weight', 'mod.blocks.0.fc2.bias', 'mod.blocks.1.c_attn', 'mod.blocks.1.w_resid', 'mod.blocks.1.pre_attn_norm.weight', 'mod.blocks.1.pre_attn_norm.bias', 'mod.blocks.1.attn.in_proj_weight', 'mod.blocks.1.attn.in_proj_bias', 'mod.blocks.1.attn.out_proj.weight', 'mod.blocks.1.attn.out_proj.bias', 'mod.blocks.1.post_attn_norm.weight', 'mod.blocks.1.post_attn_norm.bias', 'mod.blocks.1.pre_fc_norm.weight', 'mod.blocks.1.pre_fc_norm.bias', 'mod.blocks.1.fc1.weight', 'mod.blocks.1.fc1.bias', 'mod.blocks.1.post_fc_norm.weight', 'mod.blocks.1.post_fc_norm.bias', 'mod.blocks.1.fc2.weight', 'mod.blocks.1.fc2.bias', 'mod.blocks.2.c_attn', 'mod.blocks.2.w_resid', 'mod.blocks.2.pre_attn_norm.weight', 'mod.blocks.2.pre_attn_norm.bias', 'mod.blocks.2.attn.in_proj_weight', 'mod.blocks.2.attn.in_proj_bias', 'mod.blocks.2.attn.out_proj.weight', 'mod.blocks.2.attn.out_proj.bias', 'mod.blocks.2.post_attn_norm.weight', 'mod.blocks.2.post_attn_norm.bias', 'mod.blocks.2.pre_fc_norm.weight', 'mod.blocks.2.pre_fc_norm.bias', 'mod.blocks.2.fc1.weight', 'mod.blocks.2.fc1.bias', 'mod.blocks.2.post_fc_norm.weight', 'mod.blocks.2.post_fc_norm.bias', 'mod.blocks.2.fc2.weight', 'mod.blocks.2.fc2.bias', 'mod.blocks.3.c_attn', 'mod.blocks.3.w_resid', 'mod.blocks.3.pre_attn_norm.weight', 'mod.blocks.3.pre_attn_norm.bias', 'mod.blocks.3.attn.in_proj_weight', 'mod.blocks.3.attn.in_proj_bias', 'mod.blocks.3.attn.out_proj.weight', 'mod.blocks.3.attn.out_proj.bias', 'mod.blocks.3.post_attn_norm.weight', 'mod.blocks.3.post_attn_norm.bias', 'mod.blocks.3.pre_fc_norm.weight', 'mod.blocks.3.pre_fc_norm.bias', 'mod.blocks.3.fc1.weight', 'mod.blocks.3.fc1.bias', 'mod.blocks.3.post_fc_norm.weight', 'mod.blocks.3.post_fc_norm.bias', 'mod.blocks.3.fc2.weight', 'mod.blocks.3.fc2.bias', 'mod.blocks.4.c_attn', 'mod.blocks.4.w_resid', 'mod.blocks.4.pre_attn_norm.weight', 'mod.blocks.4.pre_attn_norm.bias', 'mod.blocks.4.attn.in_proj_weight', 'mod.blocks.4.attn.in_proj_bias', 'mod.blocks.4.attn.out_proj.weight', 'mod.blocks.4.attn.out_proj.bias', 'mod.blocks.4.post_attn_norm.weight', 'mod.blocks.4.post_attn_norm.bias', 'mod.blocks.4.pre_fc_norm.weight', 'mod.blocks.4.pre_fc_norm.bias', 'mod.blocks.4.fc1.weight', 'mod.blocks.4.fc1.bias', 'mod.blocks.4.post_fc_norm.weight', 'mod.blocks.4.post_fc_norm.bias', 'mod.blocks.4.fc2.weight', 'mod.blocks.4.fc2.bias', 'mod.blocks.5.c_attn', 'mod.blocks.5.w_resid', 'mod.blocks.5.pre_attn_norm.weight', 'mod.blocks.5.pre_attn_norm.bias', 'mod.blocks.5.attn.in_proj_weight', 'mod.blocks.5.attn.in_proj_bias', 'mod.blocks.5.attn.out_proj.weight', 'mod.blocks.5.attn.out_proj.bias', 'mod.blocks.5.post_attn_norm.weight', 'mod.blocks.5.post_attn_norm.bias', 'mod.blocks.5.pre_fc_norm.weight', 'mod.blocks.5.pre_fc_norm.bias', 'mod.blocks.5.fc1.weight', 'mod.blocks.5.fc1.bias', 'mod.blocks.5.post_fc_norm.weight', 'mod.blocks.5.post_fc_norm.bias', 'mod.blocks.5.fc2.weight', 'mod.blocks.5.fc2.bias', 'mod.blocks.6.c_attn', 'mod.blocks.6.w_resid', 'mod.blocks.6.pre_attn_norm.weight', 'mod.blocks.6.pre_attn_norm.bias', 'mod.blocks.6.attn.in_proj_weight', 'mod.blocks.6.attn.in_proj_bias', 'mod.blocks.6.attn.out_proj.weight', 'mod.blocks.6.attn.out_proj.bias', 'mod.blocks.6.post_attn_norm.weight', 'mod.blocks.6.post_attn_norm.bias', 'mod.blocks.6.pre_fc_norm.weight', 'mod.blocks.6.pre_fc_norm.bias', 'mod.blocks.6.fc1.weight', 'mod.blocks.6.fc1.bias', 'mod.blocks.6.post_fc_norm.weight', 'mod.blocks.6.post_fc_norm.bias', 'mod.blocks.6.fc2.weight', 'mod.blocks.6.fc2.bias', 'mod.blocks.7.c_attn', 'mod.blocks.7.w_resid', 'mod.blocks.7.pre_attn_norm.weight', 'mod.blocks.7.pre_attn_norm.bias', 'mod.blocks.7.attn.in_proj_weight', 'mod.blocks.7.attn.in_proj_bias', 'mod.blocks.7.attn.out_proj.weight', 'mod.blocks.7.attn.out_proj.bias', 'mod.blocks.7.post_attn_norm.weight', 'mod.blocks.7.post_attn_norm.bias', 'mod.blocks.7.pre_fc_norm.weight', 'mod.blocks.7.pre_fc_norm.bias', 'mod.blocks.7.fc1.weight', 'mod.blocks.7.fc1.bias', 'mod.blocks.7.post_fc_norm.weight', 'mod.blocks.7.post_fc_norm.bias', 'mod.blocks.7.fc2.weight', 'mod.blocks.7.fc2.bias', 'mod.cls_blocks.0.c_attn', 'mod.cls_blocks.0.w_resid', 'mod.cls_blocks.0.pre_attn_norm.weight', 'mod.cls_blocks.0.pre_attn_norm.bias', 'mod.cls_blocks.0.attn.in_proj_weight', 'mod.cls_blocks.0.attn.in_proj_bias', 'mod.cls_blocks.0.attn.out_proj.weight', 'mod.cls_blocks.0.attn.out_proj.bias', 'mod.cls_blocks.0.post_attn_norm.weight', 'mod.cls_blocks.0.post_attn_norm.bias', 'mod.cls_blocks.0.pre_fc_norm.weight', 'mod.cls_blocks.0.pre_fc_norm.bias', 'mod.cls_blocks.0.fc1.weight', 'mod.cls_blocks.0.fc1.bias', 'mod.cls_blocks.0.post_fc_norm.weight', 'mod.cls_blocks.0.post_fc_norm.bias', 'mod.cls_blocks.0.fc2.weight', 'mod.cls_blocks.0.fc2.bias', 'mod.cls_blocks.1.c_attn', 'mod.cls_blocks.1.w_resid', 'mod.cls_blocks.1.pre_attn_norm.weight', 'mod.cls_blocks.1.pre_attn_norm.bias', 'mod.cls_blocks.1.attn.in_proj_weight', 'mod.cls_blocks.1.attn.in_proj_bias', 'mod.cls_blocks.1.attn.out_proj.weight', 'mod.cls_blocks.1.attn.out_proj.bias', 'mod.cls_blocks.1.post_attn_norm.weight', 'mod.cls_blocks.1.post_attn_norm.bias', 'mod.cls_blocks.1.pre_fc_norm.weight', 'mod.cls_blocks.1.pre_fc_norm.bias', 'mod.cls_blocks.1.fc1.weight', 'mod.cls_blocks.1.fc1.bias', 'mod.cls_blocks.1.post_fc_norm.weight', 'mod.cls_blocks.1.post_fc_norm.bias', 'mod.cls_blocks.1.fc2.weight', 'mod.cls_blocks.1.fc2.bias', 'mod.norm.weight', 'mod.norm.bias'])\n"
     ]
    }
   ],
   "source": [
    "model_latent = helpers.load_weights(model_latent,'/Users/matthiasvigl/Xbb_lr0.01_bs512_subset0.1.pt',device)\n",
    "model_latent_hlf = helpers.load_weights(model_latent_hlf,'/Users/matthiasvigl/Xbb_lr0.01_bs512_subset0.1.pt',device)\n",
    "model_Xbb_hlf = helpers.load_weights(model_Xbb_hlf,'/Users/matthiasvigl/Xbb_lr0.01_bs512_subset0.1.pt',device)\n",
    "model_Xbb = helpers.load_weights(model_Xbb,'/Users/matthiasvigl/Xbb_lr0.01_bs512_subset0.1.pt',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_Xbb.eval()\n",
    "    preds_Xbb = model_Xbb(pf_points,pf_features,pf_vectors,pf_mask)\n",
    "    preds_Xbb_bkg = model_Xbb(pf_points_bkg,pf_features_bkg,pf_vectors_bkg,pf_mask_bkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([75.,  7.,  1.,  4.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]),\n",
       " array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "        0.99, 1.  ]),\n",
       " [<matplotlib.patches.Polygon at 0x157c443d0>])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b=np.linspace(0,1,101)\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(preds_Xbb.detach().numpy(), lw=0.8,bins=b,histtype='step', density=True, alpha=0.7)\n",
    "ax.hist(preds_Xbb_bkg.detach().numpy(), lw=0.8,bins=b,histtype='step', density=True, alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
